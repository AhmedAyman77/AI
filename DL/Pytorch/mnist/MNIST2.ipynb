{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izm9CE0WBpOe",
        "outputId": "43da4445-b323-4162-8877-f4a9e6ea34dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from google.colab import drive\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "e6g7pCQcB2Qh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYHKYVJzCThk",
        "outputId": "12fda25b-ddbc-4f1b-b72a-ace785fe2932"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 93.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 20.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 87.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.54MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION = 0.2\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "ZwD_NFzhCiBl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.arange(len(train_data))\n",
        "np.random.shuffle(idx)\n",
        "split = int(np.floor(VALIDATION * len(train_data)))\n",
        "train_idx, validation_idx = idx[split:], idx[:split]\n",
        "\n",
        "train_sample = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
        "validation_sample = torch.utils.data.sampler.SubsetRandomSampler(validation_idx)\n",
        "\n",
        "# when use sampler the shuffle is ignored\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=train_sample)\n",
        "validation_loader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=validation_sample)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "6rAhh4aqDA0i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data, target in train_loader:\n",
        "    print(data.shape, target.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_Or56mLEPfO",
        "outputId": "1cf88ad9-79ad-4086-c472-1170888d2ddf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without Dropout or Regularization"
      ],
      "metadata": {
        "id": "ijuklTpmEVwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 256)\n",
        "    self.fc2 = nn.Linear(256, 64)\n",
        "    self.fc3 = nn.Linear(64, 32)\n",
        "    self.fc4 = nn.Linear(32, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = self.fc4(x)\n",
        "    return x\n",
        "\n",
        "model = Model()"
      ],
      "metadata": {
        "id": "LPVPPkOnER5l"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "nLXOnn6hFHzP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  train_loss, valid_loss = [], []\n",
        "\n",
        "  model.train()\n",
        "  for data, target in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss_value = loss(output, target)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    train_loss.append(loss_value.item())\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for data, target in validation_loader:\n",
        "      output = model(data)\n",
        "      loss_value = loss(output, target)\n",
        "      valid_loss.append(loss_value.item())\n",
        "\n",
        "print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mehcDcJwFVZp",
        "outputId": "92516dba-229b-478f-b785-4227227779bd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9 Training Loss:  0.04912998636617946 Valid Loss:  0.10182323734067936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout:\n",
        "A simple but effective regularization technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. Dropout is again used to reduce the 'overfitting problem'. Drop is more useful when we have deep network. We give a dropout probablity(to switch off the weights randomly) in the configuration.\n",
        "\n",
        "Dropout is generally used during the training phase only and we switch off dropout during test/validation phase."
      ],
      "metadata": {
        "id": "FfdarOzfIuCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 256)\n",
        "    self.fc2 = nn.Linear(256, 64)\n",
        "    self.fc3 = nn.Linear(64, 32)\n",
        "    self.fc4 = nn.Linear(32, 10)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "    x = self.dropout(F.relu(self.fc1(x)))\n",
        "    x = self.dropout(F.relu(self.fc2(x)))\n",
        "    x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "    x = self.fc4(x)\n",
        "    return x\n",
        "\n",
        "model = Model()"
      ],
      "metadata": {
        "id": "NiE2l5tgGDus"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "KeLkU5b1JoKv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  train_loss, valid_loss = [], []\n",
        "\n",
        "  model.train()\n",
        "  for data, target in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss_value = loss(output, target)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    train_loss.append(loss_value.item())\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for data, target in validation_loader:\n",
        "      output = model(data)\n",
        "      loss_value = loss(output, target)\n",
        "      valid_loss.append(loss_value.item())\n",
        "\n",
        "  print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1gfd-QtJ1j_",
        "outputId": "ef0cc305-3336-4116-ee00-9f059dec2576"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Training Loss:  0.6221508249839147 Valid Loss:  0.25120305365070383\n",
            "Epoch: 1 Training Loss:  0.2993074818352858 Valid Loss:  0.18277823303806337\n",
            "Epoch: 2 Training Loss:  0.23591463672121366 Valid Loss:  0.16205026281680515\n",
            "Epoch: 3 Training Loss:  0.20448531943808 Valid Loss:  0.13375360706692285\n",
            "Epoch: 4 Training Loss:  0.18423983048150938 Valid Loss:  0.1379432445193859\n",
            "Epoch: 5 Training Loss:  0.16925463514402508 Valid Loss:  0.11744712773175474\n",
            "Epoch: 6 Training Loss:  0.15927540396526457 Valid Loss:  0.12152214043159434\n",
            "Epoch: 7 Training Loss:  0.14927727395047743 Valid Loss:  0.10844078564030574\n",
            "Epoch: 8 Training Loss:  0.13820888075853388 Valid Loss:  0.13125023245068385\n",
            "Epoch: 9 Training Loss:  0.13492762167255085 Valid Loss:  0.10322175402580662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Steps to note:\n",
        "\n",
        "### **torch.no_grad()**: impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop. We generally don't want backpropagation in validation and test phase.\n",
        "#### **model.eval()**: This will switch off the dropouts for validation phase.\n",
        "### **model.train()**: Will bring the model again into traning phase by switching on the dropouts.\n",
        "### If the loss of traning set and validation sets are very close that means there is less overfitting."
      ],
      "metadata": {
        "id": "f8XgywTUKFDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the network"
      ],
      "metadata": {
        "id": "zXxYMmE_Kgoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = 0\n",
        "class_correct = [0 for i in range(10)]\n",
        "class_total = [0 for i in range(10)]\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for data, target in test_loader:\n",
        "\n",
        "  output = model(data)\n",
        "\n",
        "  loss_val = loss(output, target)\n",
        "\n",
        "  # loss * BATCH_SIZE\n",
        "  test_loss += loss_val.item()*data.size(0)\n",
        "\n",
        "  _, pred = torch.max(output, 1)\n",
        "\n",
        "  correct = pred.eq(target.data.view_as(pred))\n",
        "\n",
        "  # except range(len(BATCH_SIZE)) will use range(len(target))\n",
        "  # every batch has 64 data but the last batch may has less than it which will cause an Indexing Issue\n",
        "  for i in range(len(target)):\n",
        "    label = target.data[i]\n",
        "    class_correct[label] += correct[i].item()\n",
        "    class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVryibU5J7bl",
        "outputId": "f23c0da8-53ca-458c-93e9-e11a107e51f2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.099662\n",
            "\n",
            "\n",
            "Test Accuracy (Overall): 97% (9710/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding L2 Regularization to model\n",
        "**optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)**\n",
        "##### You can specify the weight_decay lamda parameter values while defining the model optimizer.\n",
        "\n",
        "##### Higher the value of weight_decay higher the shrinkage in the model weights."
      ],
      "metadata": {
        "id": "Ihxy_jtaTrG_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDWRZ4xfMXOw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}